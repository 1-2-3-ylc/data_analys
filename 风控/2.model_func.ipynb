{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: error: error: git config user.name & please set dead value or install git && error: git config user.email & please set dead value or install git & please set dead value or install git\n",
    "Date: 2025-05-16 10:33:15\n",
    "LastEditors: error: error: git config user.name & please set dead value or install git && error: git config user.email & please set dead value or install git & please set dead value or install git\n",
    "LastEditTime: 2025-08-13 17:58:08\n",
    "FilePath: \\Project_Business\\风控\\2.model_func.ipynb\n",
    "Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "'''\n",
    "def data_report(df, numeric_missing_value = [np.nan, -999, -1111], string_missing_value = ['']):\n",
    "    '''\n",
    "    df：包含需要做统计分析变量的数据集\n",
    "    numeric_missing_value：连续型缺失值，list\n",
    "    string_missing_value：字符型缺失值，list\n",
    "    '''\n",
    "    dtypes_dict = df.dtypes.to_dict()\n",
    "    dtypes = pd.DataFrame()\n",
    "    dtypes['var'] = dtypes_dict.keys()\n",
    "    dtypes['types'] = dtypes_dict.values()\n",
    "    dtypes = dtypes[~dtypes['var'].isin(fixed_var)]\n",
    "    \n",
    "    df_string = dtypes[dtypes['types'] == 'object']\n",
    "    df_numeric = dtypes[dtypes['types'] != 'object']\n",
    "    \n",
    "    if df_numeric.shape[0] == 0:\n",
    "        print ('连续型变量个数为0，不做统计分析')\n",
    "    else:\n",
    "        numeric_cols = list(df_numeric['var'])\n",
    "        numeric_desc = pd.DataFrame()\n",
    "        for col in numeric_cols:\n",
    "            df1 = df.loc[df[col].isin(numeric_missing_value), [col]]\n",
    "            df2 = df.loc[~df[col].isin(numeric_missing_value), [col]]\n",
    "            \n",
    "            if df2.shape[0] != 0:\n",
    "                quantiles_dict = df2[col].quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).to_dict()\n",
    "                quantiles = pd.DataFrame()\n",
    "                quantiles['quantiles'] = quantiles_dict.keys()\n",
    "                quantiles[col] = quantiles_dict.values()\n",
    "                quantiles = quantiles.set_index('quantiles')\n",
    "                quantiles_T = quantiles.T\n",
    "                quantiles_T.rename(columns = {0.01:'1%', 0.05:'5%', 0.25:'25%', 0.5:'50%', 0.75:'75%', 0.95:'95%',0.99:'99%'}, inplace = True)\n",
    "            \n",
    "                quantiles_T['skew'] = df2[col].skew() #偏度\n",
    "                quantiles_T['kurt'] = df2[col].kurt()\n",
    "                quantiles_T['missing'] = df1.shape[0]\n",
    "                quantiles_T['count'] = df.shape[0]\n",
    "                quantiles_T['missing_rate'] = quantiles_T['missing'] *1.0 / quantiles_T['count']\n",
    "                quantiles_T['unique'] = len(set(df2[col]))\n",
    "                quantiles_T['min'] = df2[col].min()\n",
    "                quantiles_T['max'] = df2[col].max()\n",
    "                quantiles_T['mode'] = df2.loc[df2[col] == df2[col].mode()[0], col].count()\n",
    "                quantiles_T['mode_rate'] = quantiles_T['mode'] *1.0 / df2.shape[0]\n",
    "                quantiles_T = quantiles_T[['count', 'missing', 'missing_rate', 'unique', 'mode', 'mode_rate', 'min', '1%', '5%', '25%',\n",
    "                                           '50%', '75%', '95%', '99%', 'max', 'skew', 'kurt']]                 \n",
    "                numeric_desc = pd.concat([numeric_desc, quantiles_T])\n",
    "            else:\n",
    "                print (\"{} 无正常取值\".format(col))\n",
    "        numeric_desc = numeric_desc.reset_index()\n",
    "        numeric_desc = numeric_desc.rename(columns = {'index': 'var'})\n",
    "        numeric_desc = numeric_desc.sort_values(by = 'missing_rate', ascending = False)\n",
    "        \n",
    "    if df_string.shape[0] == 0:\n",
    "        print ('字符型变量个数为0，不做统计分析')\n",
    "    else:\n",
    "        string_cols = list(df_string['var'])\n",
    "        string_desc = pd.DataFrame()\n",
    "        for col in string_cols:\n",
    "            df1 = df.loc[df[col].isin(string_missing_value), [col]]\n",
    "            df2 = df.loc[~df[col].isin(string_missing_value), [col]]\n",
    "            \n",
    "            string = pd.DataFrame()\n",
    "            string['unique'] = [len(set(df2[col]))]\n",
    "            string['missing'] = df1.shape[0]\n",
    "            string['count'] = df.shape[0]\n",
    "            string['missing_rate'] = string['missing'] *1.0 / string['count']\n",
    "            string['var'] = col\n",
    "            string = string[['var', 'count', 'missing', 'missing_rate', 'unique']]\n",
    "            \n",
    "            string_desc = pd.concat([string_desc, string])\n",
    "    \n",
    "    if df_numeric.shape[0] == 0 and df_string.shape[0] == 0:\n",
    "        print ('无输出')\n",
    "    if df_numeric.shape[0] == 0 and df_string.shape[0] != 0:\n",
    "        return {'string_desc':string_desc}\n",
    "    if df_numeric.shape[0] != 0 and df_string.shape[0] == 0:\n",
    "        return {'numeric_desc':numeric_desc}\n",
    "    if df_numeric.shape[0] != 0 and df_string.shape[0] != 0:\n",
    "        return {'numeric_desc':numeric_desc, 'string_desc':string_desc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: error: error: git config user.name & please set dead value or install git && error: git config user.email & please set dead value or install git & please set dead value or install git\n",
    "Date: 2025-05-16 10:33:15\n",
    "LastEditors: error: error: git config user.name & please set dead value or install git && error: git config user.email & please set dead value or install git & please set dead value or install git\n",
    "LastEditTime: 2025-08-20 11:30:53\n",
    "FilePath: \\Project_Business\\风控\\2.model_func.ipynb\n",
    "Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    "'''\n",
    "def SplitData(df, col, numOfSplit, special_attribute=[]): \n",
    "    ''' \n",
    "    :param df: 按照col排序后的数据集 \n",
    "    :param col: 待分箱的变量 \n",
    "    :param numOfSplit: 切分的组别数 \n",
    "    :param special_attribute: 在切分数据集的时候，某些特殊值需要排除在外 \n",
    "    :return: 在原数据集上增加一列，把原始细粒度的col重新划分成粗粒度的值，便于分箱中的合并处理 \n",
    "    ''' \n",
    "    # 创建数据集的副本，避免修改原始数据 \n",
    "    df2 = df.copy() \n",
    "    if special_attribute != []: \n",
    "        df2 = df.loc[~df[col].isin(special_attribute)] \n",
    "    N = df2.shape[0] \n",
    "    n = int(N/numOfSplit) \n",
    "    splitPointIndex = [i*n for i in range(1,numOfSplit)] \n",
    "    rawValues = sorted(list(df2[col])) \n",
    "    splitPoint = [rawValues[i] for i in splitPointIndex] \n",
    "    splitPoint = sorted(list(set(splitPoint))) \n",
    "    return splitPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AssignGroup(x, bin):\n",
    "    '''\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param bin: 上述变量的分箱结果\n",
    "    :return: x在分箱结果下的映射\n",
    "    '''\n",
    "    N = len(bin)\n",
    "    if x<=min(bin):\n",
    "        return min(bin)\n",
    "    elif x>max(bin):\n",
    "        return 10e10 #note\n",
    "    else:\n",
    "        for i in range(N-1):\n",
    "            if bin[i] < x <= bin[i+1]:\n",
    "                return bin[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BinBadRate(df, col, target, grantRateIndicator=0):\n",
    "    '''\n",
    "    :param df: 需要计算好坏比率的数据集\n",
    "    :param col: 需要计算好坏比率的特征\n",
    "    :param target: 好坏标签\n",
    "    :param grantRateIndicator: 1返回总体的坏样本率，0不返回\n",
    "    :return: 每箱的坏样本率，以及总体的坏样本率（当grantRateIndicator＝＝1时）\n",
    "    '''\n",
    "    total = df.groupby([col])[target].count()\n",
    "    total = pd.DataFrame({'total': total})\n",
    "    bad = df.groupby([col])[target].sum()\n",
    "    bad = pd.DataFrame({'bad': bad})\n",
    "    regroup = total.merge(bad, left_index=True, right_index=True, how='left')\n",
    "#     regroup.reset_index(level=0, inplace=True)\n",
    "    regroup.reset_index(level = [0,], inplace=True)\n",
    "    regroup['bad_rate'] = regroup.apply(lambda x: x.bad * 1.0 / x.total, axis=1)\n",
    "    dicts = dict(zip(regroup[col],regroup['bad_rate']))\n",
    "    if grantRateIndicator==0:\n",
    "        return (dicts, regroup)\n",
    "    N = sum(regroup['total'])\n",
    "    B = sum(regroup['bad'])\n",
    "    overallRate = B * 1.0 / N\n",
    "    return (dicts, regroup, overallRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MergeBad0(df,col,target, direction='bad'):\n",
    "    '''\n",
    "     :param df: 包含检验0％或者100%坏样本率\n",
    "     :param col: 分箱后的变量或者类别型变量。检验其中是否有一组或者多组没有坏样本或者没有好样本。如果是，则需要进行合并\n",
    "     :param target: 目标变量，0、1表示好、坏\n",
    "     :return: 合并方案，使得每个组里同时包含好坏样本\n",
    "     '''\n",
    "    regroup = BinBadRate(df, col, target)[1]\n",
    "    if direction == 'bad':\n",
    "        # 如果是合并0坏样本率的组，则跟最小的非0坏样本率的组进行合并\n",
    "        regroup = regroup.sort_values(by  = 'bad_rate')\n",
    "    else:\n",
    "        # 如果是合并0好样本样本率的组，则跟最小的非0好样本率的组进行合并\n",
    "        regroup = regroup.sort_values(by = 'bad_rate',ascending = False)\n",
    "    regroup.index = range(regroup.shape[0])\n",
    "    col_regroup = [[i] for i in regroup[col]]\n",
    "    del_index = []\n",
    "    for i in range(regroup.shape[0]-1):\n",
    "        col_regroup[i+1] = col_regroup[i] + col_regroup[i+1]\n",
    "        del_index.append(i)\n",
    "        if direction == 'bad':\n",
    "            if regroup['bad_rate'][i+1] > 0:\n",
    "                break\n",
    "        else:\n",
    "            if regroup['bad_rate'][i+1] < 1:\n",
    "                break\n",
    "    col_regroup2 = [col_regroup[i] for i in range(len(col_regroup)) if i not in del_index]\n",
    "    newGroup = {}\n",
    "    for i in range(len(col_regroup2)):\n",
    "        for g2 in col_regroup2[i]:\n",
    "            newGroup[g2] = 'Bin '+str(i)\n",
    "    return newGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BadRateEncoding(df, col, target):\n",
    "    '''\n",
    "    :param df: dataframe containing feature and target\n",
    "    :param col: the feature that needs to be encoded with bad rate, usually categorical type\n",
    "    :param target: good/bad indicator\n",
    "    :return: the assigned bad rate to encode the categorical feature\n",
    "    '''\n",
    "    regroup = BinBadRate(df, col, target, grantRateIndicator=0)[1]\n",
    "    br_dict = regroup[[col,'bad_rate']].set_index([col]).to_dict(orient='index')\n",
    "    for k, v in br_dict.items():\n",
    "        br_dict[k] = v['bad_rate']\n",
    "    badRateEnconding = df[col].map(lambda x: br_dict[x])  #做成字典，逐一遍历映射\n",
    "    return {'encoding':badRateEnconding, 'bad_rate':br_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Chi2(df, total_col, bad_col):\n",
    "    '''\n",
    "    :param df: 包含全部样本总计与坏样本总计的数据框\n",
    "    :param total_col: 全部样本的个数\n",
    "    :param bad_col: 坏样本的个数\n",
    "    :return: 卡方值\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    # 求出df中，总体的坏样本率和好样本率\n",
    "    badRate = sum(df2[bad_col])*1.0/sum(df2[total_col])\n",
    "    df2['good'] = df2.apply(lambda x: x[total_col] - x[bad_col], axis = 1)\n",
    "    goodRate = sum(df2['good']) * 1.0 / sum(df2[total_col])\n",
    "    # 期望坏（好）样本个数＝全部样本个数*平均坏（好）样本占比\n",
    "    df2['badExpected'] = df[total_col].apply(lambda x: x*badRate)\n",
    "    df2['goodExpected'] = df[total_col].apply(lambda x: x * goodRate)\n",
    "    badCombined = zip(df2['badExpected'], df2[bad_col])\n",
    "    goodCombined = zip(df2['goodExpected'], df2['good'])\n",
    "    badChi = [(i[0]-i[1])**2/(i[0] + 0.000001) for i in badCombined]\n",
    "    goodChi = [(i[0] - i[1]) ** 2 / (i[0] + 0.000001) for i in goodCombined]\n",
    "    chi2 = sum(badChi) + sum(goodChi)\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AssignBin(x, cutOffPoints,special_attribute=[]):\n",
    "    '''\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param cutOffPoints: 上述变量的分箱结果，用切分点表示\n",
    "    :param special_attribute:  不参与分箱的特殊取值\n",
    "    :return: 分箱后的对应的第几个箱，从0开始\n",
    "    for example, if cutOffPoints = [10,20,30], if x = 7, return Bin 0. If x = 35, return Bin 3\n",
    "    '''\n",
    "    numBin = len(cutOffPoints) + 1\n",
    "    if x in special_attribute:\n",
    "        i = special_attribute.index(x) + 1\n",
    "        return 'Bin {}'.format(0-i)\n",
    "    if x <= cutOffPoints[0]:\n",
    "        return 'Bin 0'\n",
    "    elif x > cutOffPoints[-1]:\n",
    "        return 'Bin {}'.format(numBin-1)\n",
    "    else:\n",
    "        for i in range(0,numBin-1):\n",
    "            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:\n",
    "                return 'Bin {}'.format(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AssignNum(x, cutOffPoints):\n",
    "    '''\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param cutOffPoints: 上述变量的分箱结果，用切分点表示\n",
    "    :return: 分箱后的对应的第几个箱，从0开始\n",
    "    '''\n",
    "    numBin = len(cutOffPoints) + 1\n",
    "    if x >= cutOffPoints[0] and x <= cutOffPoints[1]:\n",
    "        return '{}'.format(0)\n",
    "    elif x > cutOffPoints[-1]:\n",
    "        return '{}'.format(numBin-2)\n",
    "    else:\n",
    "        for i in range(1,numBin-2):\n",
    "            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:\n",
    "                return '{}'.format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AssignNum_0(x, cutOffPoints):\n",
    "    '''\n",
    "    :0单独为一组\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param cutOffPoints: 上述变量的分箱结果，用切分点表示\n",
    "    :return: 分箱后的对应的第几个箱，从0开始\n",
    "    '''\n",
    "    numBin = len(cutOffPoints) + 1\n",
    "    if x <= cutOffPoints[0]:\n",
    "        return '{}'.format(0)\n",
    "    elif x > cutOffPoints[-1]:\n",
    "        return '{}'.format(numBin-1)\n",
    "    else:\n",
    "        for i in range(0,numBin-1):\n",
    "            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:\n",
    "                return '{}'.format(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AssignBin_me(x, cutOffPoints,special_attribute=[]):\n",
    "    '''\n",
    "    :param x: 某个变量的某个取值\n",
    "    :param cutOffPoints: 上述变量的分箱结果，用切分点表示\n",
    "    :param special_attribute:  不参与分箱的特殊取值\n",
    "    :return: 分箱后的对应的第几个箱，从0开始\n",
    "    for example, if cutOffPoints = [10,20,30], if x = 7, return Bin 0. If x = 35, return Bin 3\n",
    "    '''\n",
    "    numBin = len(cutOffPoints) + 1\n",
    "    if x in special_attribute:\n",
    "        i = special_attribute.index(x) + 1\n",
    "        return '[{},{}]'.format(int(x), int(x))   #注意返回值\n",
    "    if x <= cutOffPoints[0] and x >= 0:\n",
    "        return '[0,{}]'.format(cutOffPoints[0])\n",
    "    elif x > cutOffPoints[-1]:\n",
    "        return '({},inf)'.format(cutOffPoints[-1])\n",
    "    else:\n",
    "        for i in range(0,numBin-1):\n",
    "            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:\n",
    "                return '({},{}]'.format(cutOffPoints[i],cutOffPoints[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ChiMerge(df, col, target, max_interval=5,special_attribute=[],minBinPcnt=0):\n",
    "    '''\n",
    "    :param df: 包含目标变量与分箱属性的数据框\n",
    "    :param col: 需要分箱的属性\n",
    "    :param target: 目标变量，取值0或1\n",
    "    :param max_interval: 最大分箱数。如果原始属性的取值个数低于该参数，不执行这段函数\n",
    "    :param special_attribute: 不参与分箱的属性取值\n",
    "    :param minBinPcnt：最小箱的占比，默认为0\n",
    "    :return: 分箱结果\n",
    "    '''\n",
    "    colLevels = sorted(list(set(df[col])))\n",
    "    N_distinct = len(colLevels)\n",
    "    if len(special_attribute)>= 1: \n",
    "        df1 = df.loc[df[col].isin(special_attribute)]\n",
    "        df2 = df.loc[~df[col].isin(special_attribute)]\n",
    "    else:\n",
    "        df2 = df.copy()\n",
    "    N_distinct = len(list(set(df2[col])))\n",
    "    if N_distinct > 100:\n",
    "        split_x = SplitData(df2, col, 100)\n",
    "        df2['temp'] = df2[col].map(lambda x: AssignGroup(x, split_x))\n",
    "    else:\n",
    "        df2['temp'] = df2[col]\n",
    "        # 计算每个分组的坏样本率，并获取总体的坏样本率。\n",
    "    (binBadRate, regroup, overallRate) = BinBadRate(df2, 'temp', target, grantRateIndicator=1)\n",
    "    # 首先，每个单独的属性值将被分为单独的一组\n",
    "    # 对属性值进行排序，然后两两组别进行合并\n",
    "    colLevels = sorted(list(set(df2['temp'])))\n",
    "    groupIntervals = [[i] for i in colLevels]\n",
    "    # 步骤一：建立循环，不断合并最优的相邻两个组别，直到 最终分裂出来的分箱数<＝预设的最大分箱数\n",
    "    if N_distinct > max_interval:\n",
    "#         if len(special_attribute) == 2 and df1.shape[0] > 0:  # note\n",
    "        if len(special_attribute) == 4 and df1.shape[0] > 0:  # note\n",
    "#             split_intervals = max_interval - len(special_attribute) + 1 #  note  \n",
    "            split_intervals = max_interval - len(special_attribute) + 3 #  note  \n",
    "        else:\n",
    "            split_intervals = max_interval - len(special_attribute)\n",
    "        \n",
    "        while (len(groupIntervals) > split_intervals):  # 终止条件: 当前分箱数＝预设的分箱数\n",
    "            # 每次循环时, 计算合并相邻组别后的卡方值。具有最小卡方值的合并方案，是最优方案\n",
    "            chisqList = []\n",
    "            for k in range(len(groupIntervals)-1):\n",
    "                temp_group = groupIntervals[k] + groupIntervals[k+1]\n",
    "                df2b = regroup.loc[regroup['temp'].isin(temp_group)]\n",
    "                #chisq = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq = Chi2(df2b, 'total', 'bad')\n",
    "                chisqList.append(chisq)\n",
    "            best_comnbined = chisqList.index(min(chisqList))\n",
    "            groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined+1]\n",
    "            # after combining two intervals, we need to remove one of them\n",
    "            groupIntervals.remove(groupIntervals[best_comnbined + 1])\n",
    "        groupIntervals = [sorted(i) for i in groupIntervals]\n",
    "        cutOffPoints = [max(i) for i in groupIntervals[:-1]]\n",
    "    else:\n",
    "        cutOffPoints = colLevels[:-1]\n",
    "    #步骤二：检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本\n",
    "    groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "    df2['temp_Bin'] = groupedvalues\n",
    "    (binBadRate,regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "    groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "    df2['temp_Bin'] = groupedvalues\n",
    "    [minBadRate, maxBadRate] = [min(binBadRate.values()),max(binBadRate.values())]\n",
    "    while minBadRate ==0 or maxBadRate == 1:\n",
    "        # 找出全部为好／坏样本的箱\n",
    "        indexForBad01 = regroup[regroup['bad_rate'].isin([0,1])].temp_Bin.tolist()\n",
    "        bin=indexForBad01[0]\n",
    "        #以下这种情况自己写(原来就只有1个切分点的情况)\n",
    "        if  len(cutOffPoints) == 1 and bin == max(regroup.temp_Bin):\n",
    "            cutOffPoints = [df2['temp'].max()]\n",
    "            groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "            df2['temp_Bin'] = groupedvalues\n",
    "            (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', 'y')\n",
    "            [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "            break\n",
    "        #以上这种情况自己写\n",
    "        # 如果是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除\n",
    "        if bin == max(regroup.temp_Bin):\n",
    "            cutOffPoints = cutOffPoints[:-1]\n",
    "        # 如果是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除\n",
    "        elif bin == min(regroup.temp_Bin):\n",
    "            cutOffPoints = cutOffPoints[1:]\n",
    "        # 如果是中间的某一箱，则需要和前后中的一个箱进行合并，依据是较小的卡方值\n",
    "        else:\n",
    "            # 和前一箱进行合并，并且计算卡方值\n",
    "            currentIndex = list(regroup.temp_Bin).index(bin)\n",
    "            prevIndex = list(regroup.temp_Bin)[currentIndex - 1]\n",
    "            df3 = df2.loc[df2['temp_Bin'].isin([prevIndex, bin])]\n",
    "            (binBadRate, df2b) = BinBadRate(df3, 'temp_Bin', target)\n",
    "            #chisq1 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "            chisq1 = Chi2(df2b, 'total', 'bad')\n",
    "            # 和后一箱进行合并，并且计算卡方值\n",
    "            laterIndex = list(regroup.temp_Bin)[currentIndex + 1]\n",
    "            df3b = df2.loc[df2['temp_Bin'].isin([laterIndex, bin])]\n",
    "            (binBadRate, df2b) = BinBadRate(df3b, 'temp_Bin', target)\n",
    "            #chisq2 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "            chisq2 = Chi2(df2b, 'total', 'bad')\n",
    "            if chisq1 < chisq2:\n",
    "                cutOffPoints.remove(cutOffPoints[currentIndex - 1])\n",
    "            else:\n",
    "                cutOffPoints.remove(cutOffPoints[currentIndex])\n",
    "        # 完成合并之后，需要再次计算新的分箱准则下，每箱是否同时包含好坏样本\n",
    "        groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))     \n",
    "        df2['temp_Bin'] = groupedvalues\n",
    "        (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "        [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "        #以下这种情况自己写(合并到最后只有1个切分点的情况)\n",
    "        if  len(cutOffPoints) == 1 and len(indexForBad01) > 1:\n",
    "            cutOffPoints = [df2['temp'].max()]\n",
    "            groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "            df2['temp_Bin'] = groupedvalues\n",
    "            (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', 'y')\n",
    "            [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "            break\n",
    "        #以上这种情况自己写  \n",
    "    #步骤三：每箱的占比不低于预设值（可选）\n",
    "    # 如果有特殊属性，那么最终分裂出来的分箱数＝预设的最大分箱数－特殊属性的个数\n",
    "    # 需要检查分箱后的最小占比\n",
    "    if minBinPcnt > 0:\n",
    "        groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "        df2['temp_Bin'] = groupedvalues\n",
    "        valueCounts = groupedvalues.value_counts().to_frame()\n",
    "        valueCounts['pcnt'] = valueCounts['temp'].apply(lambda x: x * 1.0 / N)\n",
    "        valueCounts = valueCounts.sort_index()\n",
    "        minPcnt = min(valueCounts['pcnt'])\n",
    "        while minPcnt < minBinPcnt and len(cutOffPoints) > 2:\n",
    "            # 找出占比最小的箱\n",
    "            indexForMinPcnt = valueCounts[valueCounts['pcnt'] == minPcnt].index.tolist()[0]\n",
    "            # 如果占比最小的箱是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除\n",
    "            if indexForMinPcnt == max(valueCounts.index):\n",
    "                cutOffPoints = cutOffPoints[:-1]\n",
    "            # 如果占比最小的箱是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除\n",
    "            elif indexForMinPcnt == min(valueCounts.index):\n",
    "                cutOffPoints = cutOffPoints[1:]\n",
    "            # 如果占比最小的箱是中间的某一箱，则需要和前后中的一个箱进行合并，依据是较小的卡方值\n",
    "            else:\n",
    "                # 和前一箱进行合并，并且计算卡方值\n",
    "                currentIndex = list(valueCounts.index).index(indexForMinPcnt)\n",
    "                prevIndex = list(valueCounts.index)[currentIndex - 1]\n",
    "                df3 = df2.loc[df2['temp_Bin'].isin([prevIndex, indexForMinPcnt])]\n",
    "                (binBadRate, df2b) = BinBadRate(df3, 'temp_Bin', target)\n",
    "                #chisq1 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq1 = Chi2(df2b, 'total', 'bad')\n",
    "                # 和后一箱进行合并，并且计算卡方值\n",
    "                laterIndex = list(valueCounts.index)[currentIndex + 1]\n",
    "                df3b = df2.loc[df2['temp_Bin'].isin([laterIndex, indexForMinPcnt])]\n",
    "                (binBadRate, df2b) = BinBadRate(df3b, 'temp_Bin', target)\n",
    "                #chisq2 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq2 = Chi2(df2b, 'total', 'bad')\n",
    "                if chisq1 < chisq2:\n",
    "                    cutOffPoints.remove(cutOffPoints[currentIndex - 1])\n",
    "                else:\n",
    "                    cutOffPoints.remove(cutOffPoints[currentIndex])\n",
    "    cutOffPoints = special_attribute + cutOffPoints\n",
    "    return cutOffPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改过的ChiMerge;\n",
    "def ChiMerge(df, col, target, max_interval=5, special_attribute=[], minBinPcnt=0):\n",
    "    colLevels = sorted(list(set(df[col])))\n",
    "    N_distinct = len(colLevels)\n",
    "    if len(special_attribute) >= 1:\n",
    "        df1 = df.loc[df[col].isin(special_attribute)]\n",
    "        df2 = df.loc[~df[col].isin(special_attribute)]\n",
    "    else:\n",
    "        df2 = df.copy()\n",
    "    N_distinct = len(list(set(df2[col])))\n",
    "    if N_distinct > 50:\n",
    "        split_x = SplitData(df2, col, 50)\n",
    "        df2['temp'] = df2[col].map(lambda x: AssignGroup(x, split_x))\n",
    "    else:\n",
    "        df2['temp'] = df2[col]\n",
    "    (binBadRate, regroup, overallRate) = BinBadRate(df2, 'temp', target, grantRateIndicator=1)\n",
    "\n",
    "    colLevels = sorted(list(set(df2['temp'])))\n",
    "    groupIntervals = [[i] for i in colLevels]\n",
    "\n",
    "    if N_distinct > max_interval:\n",
    "        if len(special_attribute) == 2 and df1.shape[0] > 0:  # note\n",
    "            split_intervals = max_interval - len(special_attribute) + 1  # note\n",
    "        else:\n",
    "            split_intervals = max_interval - len(special_attribute)\n",
    "\n",
    "        while (len(groupIntervals) > split_intervals):  # 终止条件: 当前分箱数＝预设的分箱数\n",
    "\n",
    "            chisqList = []\n",
    "            for k in range(len(groupIntervals) - 1):\n",
    "                temp_group = groupIntervals[k] + groupIntervals[k + 1]\n",
    "                df2b = regroup.loc[regroup['temp'].isin(temp_group)]\n",
    "                # chisq = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq = Chi2(df2b, 'total', 'bad')\n",
    "                chisqList.append(chisq)\n",
    "            best_comnbined = chisqList.index(min(chisqList))\n",
    "            groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined + 1]\n",
    "            # after combining two intervals, we need to remove one of them\n",
    "            groupIntervals.remove(groupIntervals[best_comnbined + 1])\n",
    "        groupIntervals = [sorted(i) for i in groupIntervals]\n",
    "        cutOffPoints = [max(i) for i in groupIntervals[:-1]]\n",
    "    else:\n",
    "        cutOffPoints = colLevels[:-1]\n",
    "\n",
    "    groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "    df2['temp_Bin'] = groupedvalues\n",
    "    (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "    groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "    df2['temp_Bin'] = groupedvalues\n",
    "    [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "    while minBadRate == 0 or maxBadRate == 1:\n",
    "        # 找出全部为好／坏样本的箱\n",
    "        indexForBad01 = regroup[regroup['bad_rate'].isin([0, 1])].temp_Bin.tolist()\n",
    "        bin = indexForBad01[0]\n",
    "\n",
    "        if len(cutOffPoints) == 1 and bin == max(regroup.temp_Bin):\n",
    "            cutOffPoints = [df2['temp'].max()]\n",
    "            groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "            df2['temp_Bin'] = groupedvalues\n",
    "            (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "            [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "            break\n",
    "\n",
    "        if bin == max(regroup.temp_Bin):\n",
    "            cutOffPoints = cutOffPoints[:-1]\n",
    "\n",
    "        elif bin == min(regroup.temp_Bin):\n",
    "            cutOffPoints = cutOffPoints[1:]\n",
    "\n",
    "        else:\n",
    "\n",
    "            currentIndex = list(regroup.temp_Bin).index(bin)\n",
    "            prevIndex = list(regroup.temp_Bin)[currentIndex - 1]\n",
    "            df3 = df2.loc[df2['temp_Bin'].isin([prevIndex, bin])]\n",
    "            (binBadRate, df2b) = BinBadRate(df3, 'temp_Bin', target)\n",
    "            # chisq1 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "            chisq1 = Chi2(df2b, 'total', 'bad')\n",
    "\n",
    "            laterIndex = list(regroup.temp_Bin)[currentIndex + 1]\n",
    "            df3b = df2.loc[df2['temp_Bin'].isin([laterIndex, bin])]\n",
    "            (binBadRate, df2b) = BinBadRate(df3b, 'temp_Bin', target)\n",
    "            # chisq2 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "            chisq2 = Chi2(df2b, 'total', 'bad')\n",
    "            if chisq1 < chisq2:\n",
    "                cutOffPoints.remove(cutOffPoints[currentIndex - 1])\n",
    "            else:\n",
    "                cutOffPoints.remove(cutOffPoints[currentIndex])\n",
    "\n",
    "        groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "        df2['temp_Bin'] = groupedvalues\n",
    "        (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "        [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "\n",
    "        if len(cutOffPoints) == 1 and len(indexForBad01) > 1:\n",
    "            cutOffPoints = [df2['temp'].max()]\n",
    "            groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "            df2['temp_Bin'] = groupedvalues\n",
    "            (binBadRate, regroup) = BinBadRate(df2, 'temp_Bin', target)\n",
    "            [minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]\n",
    "            break\n",
    "\n",
    "    if minBinPcnt > 0:\n",
    "        groupedvalues = df2['temp'].apply(lambda x: AssignBin(x, cutOffPoints))\n",
    "        df2['temp_Bin'] = groupedvalues\n",
    "        valueCounts = groupedvalues.value_counts().to_frame()\n",
    "        valueCounts['pcnt'] = valueCounts['temp'].apply(lambda x: x * 1.0 / N)\n",
    "        valueCounts = valueCounts.sort_index()\n",
    "        minPcnt = min(valueCounts['pcnt'])\n",
    "        while minPcnt < minBinPcnt and len(cutOffPoints) > 2:\n",
    "\n",
    "            indexForMinPcnt = valueCounts[valueCounts['pcnt'] == minPcnt].index.tolist()[0]\n",
    "\n",
    "            if indexForMinPcnt == max(valueCounts.index):\n",
    "                cutOffPoints = cutOffPoints[:-1]\n",
    "\n",
    "            elif indexForMinPcnt == min(valueCounts.index):\n",
    "                cutOffPoints = cutOffPoints[1:]\n",
    "\n",
    "            else:\n",
    "\n",
    "                currentIndex = list(valueCounts.index).index(indexForMinPcnt)\n",
    "                prevIndex = list(valueCounts.index)[currentIndex - 1]\n",
    "                df3 = df2.loc[df2['temp_Bin'].isin([prevIndex, indexForMinPcnt])]\n",
    "                (binBadRate, df2b) = BinBadRate(df3, 'temp_Bin', target)\n",
    "                # chisq1 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq1 = Chi2(df2b, 'total', 'bad')\n",
    "\n",
    "                laterIndex = list(valueCounts.index)[currentIndex + 1]\n",
    "                df3b = df2.loc[df2['temp_Bin'].isin([laterIndex, indexForMinPcnt])]\n",
    "                (binBadRate, df2b) = BinBadRate(df3b, 'temp_Bin', target)\n",
    "                # chisq2 = Chi2(df2b, 'total', 'bad', overallRate)\n",
    "                chisq2 = Chi2(df2b, 'total', 'bad')\n",
    "                if chisq1 < chisq2:\n",
    "                    cutOffPoints.remove(cutOffPoints[currentIndex - 1])\n",
    "                else:\n",
    "                    cutOffPoints.remove(cutOffPoints[currentIndex])\n",
    "    cutOffPoints = special_attribute + cutOffPoints\n",
    "    return cutOffPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 判断某变量的坏样本率是否单调\n",
    "def BadRateMonotone(df, sortByVar, target,special_attribute = []):\n",
    "    '''\n",
    "    :param df: 包含检验坏样本率的变量，和目标变量\n",
    "    :param sortByVar: 需要检验坏样本率的变量\n",
    "    :param target: 目标变量，0、1表示好、坏\n",
    "    :param special_attribute: 不参与检验的特殊值\n",
    "    :return: 坏样本率单调与否\n",
    "    '''\n",
    "    df2 = df.loc[~df[sortByVar].isin(special_attribute)]\n",
    "    if len(set(df2[sortByVar])) <= 2:\n",
    "        return True\n",
    "    regroup = BinBadRate(df2, sortByVar, target)[1]\n",
    "    combined = zip(regroup['total'],regroup['bad'])\n",
    "    badRate = [x[1]*1.0/x[0] for x in combined]\n",
    "    badRateNotMonotone = [badRate[i]<badRate[i+1] and badRate[i] < badRate[i-1] or badRate[i]>badRate[i+1] and badRate[i] > badRate[i-1]\n",
    "                       for i in range(1,len(badRate)-1)]\n",
    "    if True in badRateNotMonotone:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CalcWOE(df, col, target):\n",
    "    '''\n",
    "    :param df: 包含需要计算WOE的变量和目标变量\n",
    "    :param col: 需要计算WOE、IV的变量，必须是分箱后的变量，或者不需要分箱的类别型变量\n",
    "    :param target: 目标变量，0、1表示好、坏\n",
    "    :return: 返回WOE和IV、变量每个分箱的明细\n",
    "    '''\n",
    "    total = df.groupby([col])[target].count()\n",
    "    total = pd.DataFrame({'total': total})\n",
    "    bad = df.groupby([col])[target].sum()\n",
    "    bad = pd.DataFrame({'bad': bad})\n",
    "    regroup = total.merge(bad, left_index=True, right_index=True, how='left')\n",
    "    regroup.reset_index(level=0, inplace=True)\n",
    "    N = sum(regroup['total'])\n",
    "    B = sum(regroup['bad'])\n",
    "    regroup['bad'] = regroup['bad'].map(lambda x: 0.5 if x == 0 else x)  #平滑\n",
    "    regroup['good'] = regroup['total'] - regroup['bad']\n",
    "    regroup['good'] = regroup['good'].map(lambda x: 0.5 if x == 0 else x)  #平滑\n",
    "    G = N - B\n",
    "    regroup['bad_pcnt'] = regroup['bad'].map(lambda x: x*1.0/B)\n",
    "    regroup['good_pcnt'] = regroup['good'].map(lambda x: x * 1.0 / G)\n",
    "    regroup['WOE'] = regroup.apply(lambda x: np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)\n",
    "    regroup['IV'] = regroup.apply(lambda x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)\n",
    "    regroup['var'] = col\n",
    "    regroup['bad_rate'] = regroup['bad']*1.0 / regroup['total']\n",
    "    regroup['group_rate'] = regroup['total']*1.0 / N\n",
    "    \n",
    "    WOE_dict = regroup[[col,'WOE']].set_index(col).to_dict(orient='index')\n",
    "    for k, v in WOE_dict.items():\n",
    "        WOE_dict[k] = v['WOE']\n",
    "    IV = regroup.apply(lambda x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)\n",
    "    IV = sum(IV)\n",
    "    return {\"WOE\": WOE_dict, 'IV':IV, 'regroup':regroup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_colinearity(df, high_IV_sorted, type_, correlation):\n",
    "    '''\n",
    "    df：需要分析数据集\n",
    "    high_IV_sorted：需要分析的变量及iv，list\n",
    "    iv_var_WOE：保存删去 相关系数大且IV大 的结果\n",
    "    '''\n",
    "    deleted_index = []\n",
    "    cnt_vars = len(high_IV_sorted)\n",
    "    #以下循环删去相关系数大的\n",
    "    for i in range(cnt_vars):\n",
    "        if i in deleted_index:\n",
    "            continue\n",
    "        x1 = high_IV_sorted[i][0]+\"_WOE\"\n",
    "        for j in range(cnt_vars):\n",
    "            if i == j or j in deleted_index:\n",
    "                continue\n",
    "            y1 = high_IV_sorted[j][0]+\"_WOE\"\n",
    "            roh = np.corrcoef(df[x1],df[y1])[0,1]\n",
    "            if abs(roh) > correlation:                   #阈值确定\n",
    "                x1_IV = high_IV_sorted[i][1]\n",
    "                y1_IV = high_IV_sorted[j][1]\n",
    "                if x1_IV > y1_IV:\n",
    "                    deleted_index.append(j)\n",
    "                else :\n",
    "                    deleted_index.append(i)\n",
    "    iv_var_WOE = [high_IV_sorted[i][0]+\"_WOE\" for i in range(cnt_vars) if i not in deleted_index]\n",
    "    iv_var_Bin = [high_IV_sorted[i][0] for i in range(cnt_vars) if i not in deleted_index]\n",
    "    \n",
    "    #以下跑 VIF，变量个数 >= 2\n",
    "    if len(iv_var_WOE) >= 2:\n",
    "        X = np.matrix(df[iv_var_WOE])\n",
    "        VIF_list = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        max_VIF = max(VIF_list)\n",
    "        print ('{} 的 max_VIF :{}'.format(type_, max_VIF))\n",
    "    else:\n",
    "        print('{}：只有一个变量'.format(type_))\n",
    "    return {'iv_var_WOE':iv_var_WOE, 'iv_var_Bin':iv_var_Bin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_vif(df, cols, target):\n",
    "    '''\n",
    "    df：数据集\n",
    "    cols：求vif的变量，list\n",
    "    target：因变量\n",
    "    '''\n",
    "    features = \" + \".join(cols)\n",
    "    use_df = df[cols + [target]]\n",
    "    y, x = dmatrices(target + ' ~ ' + features, use_df, return_type='dataframe')\n",
    "    data = pd.DataFrame()\n",
    "    data.loc[:, 'var'] = x.columns\n",
    "    data.loc[:, 'ols_vif'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KS(df, score, target):\n",
    "    '''\n",
    "    :param df: 包含目标变量与预测值的数据集\n",
    "    :param score: 得分或者概率\n",
    "    :param target: 目标变量\n",
    "    :return: KS值\n",
    "    '''\n",
    "    total = df.groupby([score])[target].count()\n",
    "    bad = df.groupby([score])[target].sum()\n",
    "    all = pd.DataFrame({'total':total, 'bad':bad})\n",
    "    all['good'] = all['total'] - all['bad']\n",
    "    all[score] = all.index\n",
    "    all = all.sort_values(by=score,ascending=False)\n",
    "    all.index = range(len(all))\n",
    "    all['badCumRate'] = all['bad'].cumsum() / all['bad'].sum()\n",
    "    all['goodCumRate'] = all['good'].cumsum() / all['good'].sum()\n",
    "    KS = all.apply(lambda x: x.badCumRate - x.goodCumRate, axis=1)\n",
    "    return max(KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sm_Logit(train, test, all_var_WOE, pvals_threshold = 0.1):\n",
    "    '''\n",
    "    train：训练集\n",
    "    test：测试集\n",
    "    all_var_WOE：入模变量\n",
    "    pvals_threshold：显著性阈值\n",
    "    return：返回模型变量和模型变量对应的显著性、IV值、中文解释的表\n",
    "    \n",
    "    '''\n",
    "    X_train = train[all_var_WOE] \n",
    "    y_train = train['y']\n",
    "    \n",
    "    X_test = test[all_var_WOE] \n",
    "    y_test = test['y']\n",
    "    \n",
    "    lr_var = all_var_WOE[:]\n",
    "    #模型训练\n",
    "    X_train['intercept'] = [1]*X_train.shape[0]\n",
    "    LR = sm.Logit(y_train, X_train).fit()\n",
    "    \n",
    "    #模型调整\n",
    "    pvals = LR.pvalues\n",
    "    pvals = pvals.to_dict()\n",
    "    varLargeP = {k: v for k,v in pvals.items() if v >= pvals_threshold}  #注意阈值\n",
    "    varLargeP = sorted(varLargeP.items(), key=lambda d:d[1], reverse = True)\n",
    "    \n",
    "    params = LR.params\n",
    "    params = params.to_dict()\n",
    "    params = {k: v for k,v in params.items() if v > 0}\n",
    "    params = sorted(params.items(), key=lambda d:d[1], reverse = True)\n",
    "    \n",
    "    varLargeP = varLargeP + params\n",
    "    \n",
    "    while(len(varLargeP) > 0 and len(lr_var) > 0):\n",
    "        varMaxP = varLargeP[0][0]\n",
    "        if varMaxP == 'intercept':\n",
    "            print ('the intercept is not significant!')\n",
    "            break\n",
    "        lr_var.remove(varMaxP)\n",
    "        X_train = X_train[lr_var]\n",
    "        X_train['intercept'] = [1] * X_train.shape[0]\n",
    "        LR = sm.Logit(y_train, X_train).fit()\n",
    "        \n",
    "        pvals = LR.pvalues\n",
    "        pvals = pvals.to_dict()\n",
    "        varLargeP = {k: v for k,v in pvals.items() if v >= pvals_threshold}  #注意阈值\n",
    "        varLargeP = sorted(varLargeP.items(), key=lambda d:d[1], reverse = True)\n",
    "    \n",
    "        params = LR.params\n",
    "        params = params.to_dict()\n",
    "        params = {k: v for k,v in params.items() if v > 0}\n",
    "        params = sorted(params.items(), key=lambda d:d[1], reverse = True)\n",
    "    \n",
    "        varLargeP = varLargeP + params\n",
    "        \n",
    "    LR_params = list(LR.params.index)\n",
    "    LR_params.remove('intercept')\n",
    "    summary = LR.summary()\n",
    "    print (summary)\n",
    "    \n",
    "    pvals = LR.pvalues\n",
    "    pvals = pvals.to_dict()\n",
    "    pvals_pd = pd.DataFrame()\n",
    "    pvals_pd['var'] = pvals.keys()\n",
    "    pvals_pd['p-value'] = pvals.values()\n",
    "    \n",
    "    params = LR.params\n",
    "    params = params.to_dict()\n",
    "    pvals_pd['coef'] = params.values()\n",
    "    \n",
    "    pvals_pd = pvals_pd.merge(iv_dex_pool, on = 'var', how = 'left')\n",
    "    pvals_pd = pvals_pd.drop(['long_name'], axis = 1)\n",
    "    pvals_pd = pvals_pd.rename(columns = {'var': 'var_WOE'})\n",
    "    pvals_pd['var'] = pvals_pd['var_WOE'].map(lambda x: x.replace('_WOE', ''))\n",
    "    pvals_pd = pvals_pd.sort_values(by = 'p-value', ascending = True)\n",
    "\n",
    "    y_pred = LR.predict(X_train)\n",
    "    scorecard_result = pd.DataFrame({'prob':y_pred, 'target':y_train})\n",
    "    train['y_pred'] = LR.predict(X_train)\n",
    "    print ('训练集KS：{}'.format(KS(scorecard_result,'prob','target')))\n",
    "    print ('训练集AUC：{}'.format(roc_auc_score(train['y'], train['y_pred'])))\n",
    "    \n",
    "    X_test['intercept'] = [1]*X_test.shape[0]\n",
    "    X_test = X_test[list(LR.params.index)]\n",
    "    y_pred = LR.predict(X_test)\n",
    "    test['y_pred'] = LR.predict(X_test)\n",
    "    scorecard_result = pd.DataFrame({'prob':y_pred, 'target':y_test})\n",
    "    print ('测试集KS：{}'.format(KS(scorecard_result,'prob','target')))\n",
    "    print ('测试集AUC：{}'.format(roc_auc_score(test['y'], test['y_pred'])))\n",
    "    \n",
    "    return {\"LR_params\":LR_params, \"pvals_pd\":pvals_pd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_var_detail(base_score, PDO, train, lr_params_pd, test, regroup_all):\n",
    "    \n",
    "    BaseOdds = (train['y'].value_counts()[0]) / (train['y'].value_counts()[1])\n",
    "    beta = PDO*1.0/np.log(2)\n",
    "    alpha = base_score - beta * np.log(BaseOdds)\n",
    "    model_params = list(lr_params_pd['var_WOE'])\n",
    "    model_params.remove('intercept')\n",
    "    regroup_test = pd.DataFrame()\n",
    "    \n",
    "    for col in model_params:\n",
    "        col = col.replace('_WOE', '')\n",
    "        total = test.groupby([col])['y'].count()\n",
    "        total = pd.DataFrame({'total': total})\n",
    "        total.reset_index(level=0, inplace=True)\n",
    "        total.rename(columns = {col:'Bin'}, inplace=True)\n",
    "        total['var'] = col\n",
    "        total['Bin'] = total['Bin'].astype('str')\n",
    "        \n",
    "        regroup_test = pd.concat([regroup_test, total])\n",
    "    \n",
    "    regroup_all_ = regroup_all[regroup_all['var'].isin(list(lr_params_pd['var']))]\n",
    "    \n",
    "    regroup_model_var = regroup_all_.merge(regroup_test, on = ['var', 'Bin'], how = 'outer', suffixes = ('_x', '_y'))\n",
    "#     regroup_model_var['bad_rate'] = regroup_model_var['bad']*1.0 / regroup_model_var['total_x']\n",
    "    total_x = train.shape[0]\n",
    "    total_y = test.shape[0]\n",
    "    regroup_model_var['psi'] = (regroup_model_var['total_y']*1.0 / total_y - regroup_model_var['total_x']*1.0 / total_x)*np.log((regroup_model_var['total_y']*1.0 / total_y)/(regroup_model_var['total_x']*1.0 / total_x))\n",
    "    \n",
    "    #加入每个变量每组的得分\n",
    "    intercept = list(lr_params_pd[lr_params_pd['var_WOE'] == 'intercept'].loc[:, 'coef'])[0]\n",
    "    n = lr_params_pd.shape[0] - 1\n",
    "    \n",
    "    regroup_model_var = regroup_model_var.merge(lr_params_pd, on = 'var', how = 'inner')\n",
    "    regroup_model_var['score'] = alpha/n - beta*(intercept/n + regroup_model_var['WOE']*regroup_model_var['coef'])\n",
    "    regroup_model_var['score'] = regroup_model_var['score'].map(lambda x: int(x))\n",
    "    regroup_model_var = regroup_model_var[['var', 'explain', 'Bin','bad', 'good', 'bad_rate', 'group_rate', 'bad_pcnt', 'good_pcnt', 'WOE', 'IV', 'total_x', 'total_y', 'psi', 'score']]\n",
    "    \n",
    "    return regroup_model_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KS_ROC_plot(df, y_variable, y_pred, score, title , plot_text = ''):\n",
    "    '''\n",
    "    df：输入的数据集\n",
    "    y_variable：因变量\n",
    "    y_pred：预测为1的概率\n",
    "    score：预测为1的分数\n",
    "    '''\n",
    "    print ('{}：plot'.format(title))\n",
    "    df['1_y'] = 1 - df[y_variable]\n",
    "    df_grouped = df[[y_pred, y_variable, '1_y']].groupby(y_pred).agg(\n",
    "        {y_variable: np.sum, '1_y': np.sum}).reset_index()\n",
    "    \n",
    "    sensitivity = df_grouped.sort_values(y_pred, ascending = False)\n",
    "    sensitivity['cum_sum'] = sensitivity[y_variable].cumsum()\n",
    "    sensitivity = sensitivity.sort_values('cum_sum', ascending = False)['cum_sum'] / np.sum(df[y_variable])\n",
    "    \n",
    "    specificity = df_grouped.sort_values(y_pred)\n",
    "    specificity['cum_sum'] = specificity['1_y'].cumsum()\n",
    "    specificity = specificity['cum_sum'] / (df.shape[0] - np.sum(df[y_variable]))\n",
    "    \n",
    "    auc = roc_auc_score(df['y'],df[y_pred]) \n",
    "    \n",
    "    df_grouped_ks = df[[score, y_variable, '1_y']].groupby(score).agg(\n",
    "        {y_variable: np.sum, '1_y': np.sum}).reset_index()\n",
    "    \n",
    "    df_grouped_ks['cum_pct_0'] = df_grouped_ks['1_y'].cumsum() / (\n",
    "        df.shape[0] - np.sum(df[y_variable]))\n",
    "    df_grouped_ks['cum_pct_1'] = df_grouped_ks[y_variable].cumsum() / np.sum(df[y_variable])\n",
    "    df_grouped_ks['ks'] = (df_grouped_ks['cum_pct_1'] - df_grouped_ks['cum_pct_0']) \n",
    "    max_ks = df_grouped_ks.loc[df_grouped_ks['ks'].idxmax() : df_grouped_ks['ks'].idxmax()]\n",
    "    \n",
    "    plt.plot(1 - specificity, sensitivity, linewidth = 2)\n",
    "    plt.plot([0, 1], [0, 1], color = 'grey')\n",
    "    plt.text(0.7, 0.16, plot_text, fontsize = 14)\n",
    "    plt.text(0.7, 0.1, 'RoC Curve', fontsize = 14)\n",
    "    plt.text(0.7, 0.04, 'AUC = %.4f' % auc, fontsize = 14)\n",
    "    plt.xlabel('1 - Specificity', size = 14)\n",
    "    plt.ylabel('Sensitivity', size = 14)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    fig.savefig('{} auc.png'.format(title), dpi = 100)\n",
    "    \n",
    "    plt.plot(df_grouped_ks[score], df_grouped_ks['cum_pct_0'], linewidth = 2)\n",
    "    plt.plot(df_grouped_ks[score], df_grouped_ks['cum_pct_1'], linewidth = 2)\n",
    "    plt.plot([max_ks[score].iloc[0], max_ks[score].iloc[0]], \n",
    "             [max_ks['cum_pct_0'].iloc[0], max_ks['cum_pct_1'].iloc[0]], color = 'grey')\n",
    "    plt.text(0.1 + 0.4 * np.max(df_grouped_ks[score]), np.min(df_grouped_ks['cum_pct_1']) + 0.16, \n",
    "                                plot_text, fontsize = 14)\n",
    "    \n",
    "    plt.text(500, 0.1, 'Score. Distribution', fontsize = 14)\n",
    "    plt.text(500, 0.04, 'KS = %.4f' % max_ks['ks'].iloc[0], fontsize = 14)\n",
    "    \n",
    "#     plt.text(0.1 + 0.08 * np.max(df_grouped_ks[score]), np.min(df_grouped_ks['cum_pct_1']) + 0.1, \n",
    "#                                 'Score. Distribution', fontsize = 14)\n",
    "#     plt.text(0.1 + 0.08 * np.max(df_grouped_ks[score]), np.min(df_grouped_ks['cum_pct_1']) + 0.04, \n",
    "#                                 'KS = %.4f' % max_ks['ks'].iloc[0], fontsize = 14)\n",
    "    plt.xlabel('score', size = 14)\n",
    "    plt.ylabel('Proportion', size = 14)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    fig.savefig('{} ks.png'.format(title), dpi = 100)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_group_qcut(df1, df, target, score, number_of_groups = 10):\n",
    "    '''\n",
    "    :param df_tmp: 包含目标变量与预测值的数据集，一般是训练集\n",
    "    :param df_tmp1: 需要分组统计的数据集，一般是训练集、验证集、测试集，分组和训练集的标准保持一致\n",
    "    :param score: 得分或者概率；\n",
    "    :param target: 目标变量\n",
    "    '''\n",
    "    df_tmp1 = df1.copy()\n",
    "    df_tmp = df.copy()\n",
    "    \n",
    "    df_tmp1 = df_tmp1[df_tmp1[target].notnull()]\n",
    "    df_tmp1 = df_tmp1.copy().reset_index().drop('index', axis = 1)\n",
    "    \n",
    "    quantiles = np.linspace(0, 1, number_of_groups + 1)\n",
    "    cutOffPoints = algos.quantile(df_tmp1[score], quantiles)\n",
    "    \n",
    "    df_tmp[(df_tmp[score] > cutOffPoints.max())] = cutOffPoints.max()\n",
    "    df_tmp[(df_tmp[score] < cutOffPoints.min())] = cutOffPoints.min()\n",
    "    \n",
    "    cutOffPoints = sorted(list(set(cutOffPoints)))\n",
    "\n",
    "    bins = pd.DataFrame(pd.cut(df_tmp[score], cutOffPoints)).rename(columns = {score: 'bin_group'})\n",
    "        \n",
    "    df_tmp = df_tmp.merge(bins, how = 'left', left_index = True, right_index = True)\n",
    "    \n",
    "    df_tmp['cnt_group_total'] = 1\n",
    "    df_tmp['cnt_bad'] = df_tmp[target].apply(lambda x: 1 if x == 1 else 0)\n",
    "    df_tmp['cnt_good'] = df_tmp[target].apply(lambda x: 1 if x == 0 else 0)\n",
    "    \n",
    "    df_tmp_grouped = df_tmp.groupby('bin_group').agg({'cnt_bad': np.sum, 'cnt_good': np.sum, 'cnt_group_total':np.sum})\n",
    "    df_tmp_grouped = df_tmp_grouped.reset_index()\n",
    "    \n",
    "    if df_tmp[score].max() > 1:\n",
    "        df_tmp_grouped = df_tmp_grouped.sort_values(by = 'bin_group', ascending = True)\n",
    "    else:\n",
    "        df_tmp_grouped = df_tmp_grouped.sort_values(by = 'bin_group', ascending = False)\n",
    "        \n",
    "    df_tmp_grouped['cum_bad'] = df_tmp_grouped['cnt_bad'].cumsum()\n",
    "    df_tmp_grouped['cum_good'] = df_tmp_grouped['cnt_good'].cumsum()\n",
    "    B = df_tmp_grouped['cnt_bad'].sum()\n",
    "    G = df_tmp_grouped['cnt_good'].sum()\n",
    "    df_tmp_grouped['group_bad_rate'] = df_tmp_grouped['cnt_bad'] / df_tmp_grouped['cnt_group_total']\n",
    "    df_tmp_grouped['cum_bad_pct'] = df_tmp_grouped['cum_bad'] / B\n",
    "    df_tmp_grouped['cum_good_pct'] = df_tmp_grouped['cum_good'] / G\n",
    "    df_tmp_grouped['KS'] = df_tmp_grouped['cum_bad_pct'] - df_tmp_grouped['cum_good_pct']\n",
    "    df_tmp_grouped['KS'] = df_tmp_grouped['KS'].apply(lambda x: abs(x))\n",
    "    \n",
    "    return df_tmp_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_group_qcut_score(df1, df, target, score, number_of_groups = 10):\n",
    "    '''\n",
    "    :param df_tmp: 包含目标变量与预测值的数据集，一般是训练集\n",
    "    :param df_tmp1: 需要分组统计的数据集，一般是训练集、验证集、测试集，分组和训练集的标准保持一致\n",
    "    :param score: 得分或者概率；\n",
    "    :param target: 目标变量\n",
    "    '''\n",
    "    df_tmp1 = df1[[target,score]].copy()\n",
    "    df_tmp = df.copy()\n",
    "    \n",
    "    df_tmp1 = df_tmp1[df_tmp1[target].notnull()]\n",
    "    score_max = df_tmp1[score].max()\n",
    "    score_min = df_tmp1[score].min()\n",
    "    n = (score_max - score_min)/number_of_groups\n",
    "     \n",
    "    cutOffPoints = []\n",
    "    for i in range(number_of_groups + 1):\n",
    "        bins = i * n + score_min\n",
    "        cutOffPoints.append(bins)\n",
    "    cutOffPoints = cutOffPoints[:-1]\n",
    "    cutOffPoints = cutOffPoints + [score_max]\n",
    "    \n",
    "    df_tmp[(df_tmp[score] > score_max)] = score_max\n",
    "    df_tmp[(df_tmp[score] < score_min)] = score_min\n",
    "    \n",
    "    cutOffPoints = sorted(list(set(cutOffPoints)))\n",
    "\n",
    "    bins = pd.DataFrame(pd.cut(df_tmp[score], cutOffPoints)).rename(columns = {score: 'bin_group'})\n",
    "        \n",
    "    df_tmp = df_tmp.merge(bins, how = 'left', left_index = True, right_index = True)\n",
    "    \n",
    "    df_tmp['cnt_group_total'] = 1\n",
    "    df_tmp['cnt_bad'] = df_tmp[target].apply(lambda x: 1 if x == 1 else 0)\n",
    "    df_tmp['cnt_good'] = df_tmp[target].apply(lambda x: 1 if x == 0 else 0)\n",
    "    \n",
    "    df_tmp_grouped = df_tmp.groupby('bin_group').agg({'cnt_bad': np.sum, 'cnt_good': np.sum, 'cnt_group_total':np.sum})\n",
    "    df_tmp_grouped = df_tmp_grouped.reset_index()\n",
    "    \n",
    "#     if df_tmp[score].max() > 1:\n",
    "#         df_tmp_grouped = df_tmp_grouped.sort_values(by = 'bin_group', ascending = True)\n",
    "#     else:\n",
    "#         df_tmp_grouped = df_tmp_grouped.sort_values(by = 'bin_group', ascending = True)\n",
    "        \n",
    "    df_tmp_grouped['cum_bad'] = df_tmp_grouped['cnt_bad'].cumsum()\n",
    "    df_tmp_grouped['cum_good'] = df_tmp_grouped['cnt_good'].cumsum()\n",
    "    B = df_tmp_grouped['cnt_bad'].sum()\n",
    "    G = df_tmp_grouped['cnt_good'].sum()\n",
    "    df_tmp_grouped['group_bad_rate'] = df_tmp_grouped['cnt_bad'] / df_tmp_grouped['cnt_group_total']\n",
    "    df_tmp_grouped['cum_bad_pct'] = df_tmp_grouped['cum_bad'] / B\n",
    "    df_tmp_grouped['cum_good_pct'] = df_tmp_grouped['cum_good'] / G\n",
    "    df_tmp_grouped['KS'] = df_tmp_grouped['cum_bad_pct'] - df_tmp_grouped['cum_good_pct']\n",
    "    df_tmp_grouped['KS'] = df_tmp_grouped['KS'].apply(lambda x: abs(x))\n",
    "    \n",
    "    return df_tmp_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_colinearity_ori(df, high_IV_sorted, type_, correlation):\n",
    "    '''\n",
    "    df：需要分析数据集\n",
    "    high_IV_sorted：需要分析的变量及iv，list\n",
    "    iv_var_WOE：保存删去 相关系数大且IV大 的结果\n",
    "    '''\n",
    "    deleted_index = []\n",
    "    cnt_vars = len(high_IV_sorted)\n",
    "    #以下循环删去相关系数大的\n",
    "    for i in range(cnt_vars):\n",
    "        if i in deleted_index:\n",
    "            continue\n",
    "        x1 = high_IV_sorted[i][0]\n",
    "        for j in range(cnt_vars):\n",
    "            if i == j or j in deleted_index:\n",
    "                continue\n",
    "            y1 = high_IV_sorted[j][0]\n",
    "            roh = np.corrcoef(df[x1],df[y1])[0,1]\n",
    "            if abs(roh) > correlation:                   #阈值确定\n",
    "                x1_IV = high_IV_sorted[i][1]\n",
    "                y1_IV = high_IV_sorted[j][1]\n",
    "                if x1_IV > y1_IV:\n",
    "                    deleted_index.append(j)\n",
    "                else :\n",
    "                    deleted_index.append(i)\n",
    "    iv_var_ori = [high_IV_sorted[i][0] for i in range(cnt_vars) if i not in deleted_index]\n",
    "    \n",
    "    #以下跑 VIF，变量个数 >= 2\n",
    "    if len(iv_var_ori) >= 2:\n",
    "        X = np.matrix(df[iv_var_ori])\n",
    "        VIF_list = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        max_VIF = max(VIF_list)\n",
    "        print ('{} 的 max_VIF :{}'.format(type_, max_VIF))\n",
    "    else:\n",
    "        print('{}：只有一个变量'.format(type_))\n",
    "    return {'iv_var_ori':iv_var_ori}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
